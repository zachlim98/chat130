{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install llama-index\n",
    "! pip install azure-search-documents==11.4.0b8 --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "import keys\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.schema import MetadataMode\n",
    "\n",
    "from llama_index.embeddings import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    engine=\"raidGPT\",\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    "    api_base=\"https://raid-ses-openai.openai.azure.com/\",\n",
    "    api_key=keys.gpt_key,\n",
    "    api_type=\"azure\",\n",
    "    api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "emb_llm = OpenAIEmbedding(\n",
    "    engine=\"swiftfaq-ada002\",\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    temperature=0.0,\n",
    "    api_base=\"https://raid-ses-openai.openai.azure.com/\",\n",
    "    api_key=keys.gpt_key,\n",
    "    api_type=\"azure\",\n",
    "    api_version=\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('../data/124').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "import keys\n",
    "\n",
    "from llama_index.vector_stores.cogsearch import (\n",
    "    IndexManagement,\n",
    "    CognitiveSearchVectorStore,\n",
    "    MetadataIndexFieldType\n",
    ")\n",
    "\n",
    "from llama_index import (\n",
    "    LangchainEmbedding,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    ServiceContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "\n",
    "service_endpoint = \"https://aimlexplorationsearch.search.windows.net\"\n",
    "index_name = \"rsaf-cognitive-search\"\n",
    "key = keys.cognitive_key\n",
    "credential = AzureKeyCredential(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating own nodes from azure output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import CustomQueryEngine\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from llama_index.response_synthesizers import get_response_synthesizer, BaseSynthesizer\n",
    "from llama_index.schema import Node, NodeWithScore\n",
    "from llama_index import QueryBundle\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.chat_engine import CondenseQuestionChatEngine, ContextChatEngine\n",
    "from azure.search.documents.models import Vector\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        search_client : SearchClient,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._search_client = search_client\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        nodes = []\n",
    "        # filtering\n",
    "        vector = Vector(value=emb_llm.get_text_embedding(query_bundle.query_str), fields=\"embedding\", k=6)\n",
    "        results = self._search_client.search(search_text=query_bundle, vectors=[vector], top=6)\n",
    "        # citations management\n",
    "        # possible for rights management (post filtering)\n",
    "        for i in results:\n",
    "            nodes.append(NodeWithScore(node=Node(text=i[\"content\"]), score=i['@search.score']))\n",
    "\n",
    "        return nodes\n",
    " \n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=\"rsaf-cognitive-search\", credential=credential)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=emb_llm)\n",
    "    \n",
    "chat_engine = ContextChatEngine.from_defaults(retriever=CustomRetriever(search_client=search_client), verbose=True, service_context=service_context)\n",
    "\n",
    "# response = chat_engine.chat(\"Describe how to perform an autorotation in SBAB. Provide the document name and page number for each source and put that information at the end of every generated sentence in brackets.\")\n",
    "\n",
    "response = chat_engine.chat(\"Describe what a fenestron is. Provide the document name and page number of the two main sources that was used to generate the information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OFFICIAL (CLOSED)\\nhas seven stator vanes that assist in straightening and guiding the air flow for the\\noptimum AOA on the rotor blades. The dynamic section has 8 asymmetrically\\nspaced rotor blades that rotate clockwise (when viewed from the port side) in a\\n'Union Jack' arrangement to optimize performance and reduce noise. Controlling\\nthe thrust of the Fenestron is done in the same way as a conventional tail rotor\\nsystem. The pilot's pedal inputs changes the pitch of all the tail rotor blades\\nthrough a mechanical linkage to the tail gearbox. The tail rotor design has several\\nimprovements over a regular tail rotor:\\na.\\nCreates an aerodynamically efficient design that produces smaller\\ntip vortices on the ends of the rotor blades, reducing drag which in turn,\\nmaximizes thrust produced.\\nb.\\nSignificantly reduces noise compared to a traditional tail rotor.\\nc.\\nShrouded design allows for a safer working area for crews with\\nrotors turning.\\nd.\\nShrouded design helps minimise tail rotor strikes when landing at\\nunprepared landing sites.\\nFigure 1A-2: Fenestron tail rotor\\nSTARBOARD\\nPORT\\nS/N1433\\nSTATOR\\nVANES\\nTAIL ROTOR\\nBLADES\\nS/N1444\\n1-A-2\\nOFFICIAL (CLOSED)\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API friendly version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This creates a new llamaindex compatible Azure Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_index = SearchIndexClient(endpoint=service_endpoint, credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_azure_index(index_client, index_name : str, metadata_fields : dict, llm, emb_llm, filepath) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Spins up an azure cognitive search index.\n",
    "    Will delete any index named as such so BE CAREFUL\n",
    "    \"\"\"\n",
    "    \n",
    "    vector_store = CognitiveSearchVectorStore(\n",
    "        search_or_index_client=index_client,\n",
    "        index_name=index_name,\n",
    "        filterable_metadata_field_keys=metadata_fields,\n",
    "        index_management=IndexManagement.CREATE_IF_NOT_EXISTS,\n",
    "        id_field_key=\"id\",\n",
    "        chunk_field_key=\"content\",\n",
    "        embedding_field_key=\"embedding\",\n",
    "        metadata_string_field_key=\"li_jsonMetadata\",\n",
    "        doc_id_field_key=\"li_doc_id\",\n",
    "    )\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    service_context = ServiceContext.from_defaults(llm=llm, embed_model=emb_llm)\n",
    "    \n",
    "    documents = SimpleDirectoryReader(filepath).load_data()\n",
    "    \n",
    "    ####################################\n",
    "    # Chunking/Form Intelligence here  #\n",
    "    ####################################\n",
    "    \n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, storage_context=storage_context, service_context=service_context\n",
    ")\n",
    "    \n",
    "    return \"{}\".format(index_name) + \" created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_azure_index(index_client=client_index, index_name=\"chat-demo\", metadata_fields={\"total_pages\" : (\"total_pages\", MetadataIndexFieldType.INT32),\n",
    "                                                                                       \"page\" : (\"page\", MetadataIndexFieldType.INT32),\n",
    "                                                                                       \"filename\" : (\"filename\", MetadataIndexFieldType.STRING)}, \n",
    "                   llm = llm, emb_llm=emb_llm, filepath=\"../data/124\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode\n",
    "\n",
    "nodes = [\n",
    "    TextNode(\n",
    "        text=\"The Shawshank Redemption\",\n",
    "        metadata={\n",
    "            \"page_number\": 23,\n",
    "            \"filename\": \"Friendship\",\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"The Godfather\",\n",
    "        metadata={\n",
    "            \"page_number\": 24,\n",
    "            \"filename\": \"Mafia\",\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Inception\",\n",
    "        metadata={\n",
    "            \"page_number\": 25,\n",
    "            \"filename\": \"Friendship\",\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "vector_store = CognitiveSearchVectorStore(\n",
    "    search_or_index_client=client_index,\n",
    "    index_name=\"nodes-test\",\n",
    "    filterable_metadata_field_keys={\"filename\" : \"filename\"},\n",
    "    index_management=IndexManagement.CREATE_IF_NOT_EXISTS,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"content\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    metadata_string_field_key=\"li_jsonMetadata\",\n",
    "    doc_id_field_key=\"li_doc_id\",\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=emb_llm)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "        nodes, storage_context=storage_context, service_context=service_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing custom filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"quickstart\"\n",
    "\n",
    "metadata_fields = {\n",
    "    \"page_label\": \"page_label\",\n",
    "    \"theme\": \"theme\",\n",
    "    \"director\": \"director\",\n",
    "}\n",
    "\n",
    "client_index = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "\n",
    "vector_store = CognitiveSearchVectorStore(\n",
    "    search_or_index_client=client_index,\n",
    "    index_name=index_name,\n",
    "    filterable_metadata_field_keys=metadata_fields,\n",
    "    index_management=IndexManagement.CREATE_IF_NOT_EXISTS,\n",
    "    id_field_key=\"id\",\n",
    "    chunk_field_key=\"content\",\n",
    "    embedding_field_key=\"embedding\",\n",
    "    metadata_string_field_key=\"li_jsonMetadata\",\n",
    "    doc_id_field_key=\"li_doc_id\",\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=emb_llm)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "        documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This creates a custom retriever with a chat engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.chat_engine import CondenseQuestionChatEngine, ContextChatEngine\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        search_client : SearchClient,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._search_client = search_client\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        nodes = []\n",
    "        # filtering\n",
    "        results = self._search_client.search(search_text=query_bundle, top=3)\n",
    "        # citations management\n",
    "        # possible for rights management (post filtering)\n",
    "        for i in results:\n",
    "            nodes.append(NodeWithScore(node=Node(text=i[\"content\"]), score=i['@search.score']))\n",
    "\n",
    "        return nodes\n",
    " \n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=\"chat-demo\", credential=credential)\n",
    "    \n",
    "chat_engine = ContextChatEngine.from_defaults(retriever=CustomRetriever(search_client=search_client), verbose=True, service_context=service_context)\n",
    "\n",
    "chat_engine.chat(\"What are advanced transitions?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This allows you to query that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom class that calls on search client\n",
    "class AzureQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"Azure Custom Query.\"\"\"\n",
    "\n",
    "    search_client : SearchClient\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "    \n",
    "    def custom_query(self, query_str: str):\n",
    "        \n",
    "        nodes = []\n",
    "        # filtering\n",
    "        results = self.search_client.search(search_text=query_str, top=3)\n",
    "        # citations management\n",
    "        # possible for rights management (post filtering)\n",
    "        for i in results:\n",
    "            nodes.append(NodeWithScore(node=Node(text=i[\"content\"]), score=i['@search.score']))\n",
    "\n",
    "        response_obj = self.response_synthesizer.synthesize(query_str, nodes)\n",
    "        return response_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_azure_query_engine(service_endpoint, llm, emb_llm):\n",
    "    \n",
    "    service_context = ServiceContext.from_defaults(llm=llm, embed_model=emb_llm)\n",
    "\n",
    "    search_client = SearchClient(endpoint=service_endpoint, index_name=\"llamaindex-demo\", credential=credential)\n",
    "    \n",
    "    synthesizer = get_response_synthesizer(response_mode=\"compact\", service_context=service_context)\n",
    "    query_engine = AzureQueryEngine(search_client=search_client, response_synthesizer=synthesizer)\n",
    "\n",
    "    return query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
