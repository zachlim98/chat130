{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "! pip install openai num2words matplotlib plotly scipy scikit-learn pandas tiktoken langchain pypdf faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup openAI connections\n",
    "\n",
    "import keys\n",
    "\n",
    "embed_key = keys.embed_key\n",
    "embed_endpoint = \"https://raid-openai-e27bcf212.openai.azure.com/\"\n",
    "\n",
    "gpt_key = keys.gpt_key\n",
    "gpt_endpoint = \"https://raid-ses-openai.openai.azure.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some logic here to better split the documents instead of just by page??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "def load_docs(filepath):\n",
    "    loader = DirectoryLoader(filepath, glob='**/*.pdf', loader_cls=PyPDFLoader)\n",
    "\n",
    "    docs = loader.load()\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def vector_load(docs, key, endpoint):\n",
    "    \n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "    openai_api_type=\"azure\",\n",
    "    openai_api_key=key, \n",
    "    openai_api_base=endpoint,\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    deployment=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    \n",
    "    # need to build some logic here for checking the database - if exists then just add if not, create\n",
    "    \n",
    "    db = FAISS.from_documents(docs, embedding_model)\n",
    "    \n",
    "    return db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_docs('./data/')\n",
    "db = vector_load(docs, embed_key, embed_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(\"dbstore\", OpenAIEmbeddings(\n",
    "    openai_api_type=\"azure\",\n",
    "    openai_api_key=embed_key, \n",
    "    openai_api_base=embed_endpoint,\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    deployment=\"text-embedding-ada-002\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# initiate llm\n",
    "llm = AzureChatOpenAI(openai_api_type=\"azure\", \n",
    "                      openai_api_version=\"2023-05-15\", \n",
    "                      openai_api_base=gpt_endpoint, \n",
    "                      openai_api_key=gpt_key, \n",
    "                      deployment_name=\"raidGPT\", \n",
    "                      temperature=0.0)\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs = {\"k\": 10})\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm,\n",
    "                                   memory_key=\"chat_history\", \n",
    "                                   input_key=\"question\", \n",
    "                                   output_key=\"answer\", \n",
    "                                   return_messages=True)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, \n",
    "                                           retriever=retriever, \n",
    "                                           return_source_documents=True, \n",
    "                                           memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa({'question' : \"What is a FOEL check?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\PC-21 Employment Manual - Chapt 10 Straight _ Level AL16 Mar 2023.pdf\n"
     ]
    }
   ],
   "source": [
    "print(result[\"source_documents\"][0].metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_template = \"\"\"\n",
    "You are a bot designed to answer military pilot trainees' questions from various flying handbooks and rulebooks. Use the context provided below to answer their questions. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "\n",
    "{context}\n",
    "\n",
    "Additionally, this was the chat history of your conversation with the user.\n",
    "{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(template=custom_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, \n",
    "                                           retriever=retriever, \n",
    "                                           return_source_documents=True, \n",
    "                                           memory = memory,\n",
    "                                           combine_docs_chain_kwargs={\"prompt\" : PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa({'question' : \"What is the recommended height for a normal circuit?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The recommended altitude for a normal flight circuit is 1000 feet Above Ground Level (AGL), as mentioned in section 4.1.1 of Chapter 15.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
